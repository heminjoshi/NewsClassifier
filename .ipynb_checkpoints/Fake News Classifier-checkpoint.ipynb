{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naming Conventions\n",
    "models/ \n",
    "    ../ ('..' is the Task Name. For example : News/ will contain all the models trained for News Classification \n",
    "        ../ ( '..' is the Model Name. For examle : SVM/ will contain all Support Vector Machine Models)\n",
    "\n",
    "    Example :\n",
    "    Toxic/\n",
    "        SVM/ (All the SVM's trained)\n",
    "        RF/ (All the RF's trained)\n",
    "        NV/ (All the NB's trained) [As of now, I am only using MNB. \n",
    "\n",
    "vectors/\n",
    "    News/\n",
    "        TF-IDF Vectors\n",
    "    Toxic/\n",
    "        TF-IDF Vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "To use any Vector, Models with the same **ID** number (after the model name) **MUST** be used. Any other, and it will fail.\n",
    "\n",
    "Ex : vector-1.tfidf will work with model-1.pl\n",
    "If you try model-2, then it will result in an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation / Plotting\n",
    "import seaborn as sea\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature Extractions / Preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "# Model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metric\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Data Structure\n",
    "import string \n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training for Fake vs Real\n",
    "*Dataset can be found [here](https://www.kaggle.com/anthonyc1/fake-news-classifier-final-project/data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "real    15712\n",
       "fake    12999\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:/Academic/Data Science/nc_data/news/news_dataset.csv\", encoding='utf-8')\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>publication</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
       "      <td>Print They should pay all the back all the mon...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
       "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
       "      <td>Red State : \\nFox News Sunday reported this mo...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
       "      <td>Email Kayla Mueller was a prisoner and torture...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
       "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
       "      <td>100percentfedup</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
       "1           1  Re: Why Did Attorney General Loretta Lynch Ple...   \n",
       "2           2  BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
       "3           3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
       "4           4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
       "\n",
       "                                             content      publication label  \n",
       "0  Print They should pay all the back all the mon...  100percentfedup  fake  \n",
       "1  Why Did Attorney General Loretta Lynch Plead T...  100percentfedup  fake  \n",
       "2  Red State : \\nFox News Sunday reported this mo...  100percentfedup  fake  \n",
       "3  Email Kayla Mueller was a prisoner and torture...  100percentfedup  fake  \n",
       "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  100percentfedup  fake  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping null values for now.\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hemin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hemin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "real, fake = df.loc[df.label == 'real'], df.loc[df.label == 'fake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF \n",
    "```\n",
    "    Training/Testing on CONTENT only. Title is not used.\n",
    "    Possible ways of training.\n",
    "    1. Direct TF-IDF (Baseline and Benchmark)\n",
    "    2. Word Tokenize and TF-IDF\n",
    "    3. Word Tokenize, Lemmatize, Remove Punctuation and TF-IDF\n",
    "```\n",
    "### Models Used\n",
    "__*When testing, please test on REAL data as well. High model performance does not mean that the model will work good on Real News samples*__\n",
    "``` \n",
    "    1. Linear SVC (Baseline and Benchmark) \n",
    "    2. Multinomial Naive Bayes\n",
    "    3. Logistic Regression\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Real and Fake samples\n",
    "real_content = list(real.content)\n",
    "fake_content = list(fake.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Train and Test size (per class)\n",
    "train_size = 10000\n",
    "test_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "x_train = real_content[:train_size] + fake_content[:train_size]\n",
    "y_train = [1 for _ in range(train_size)] + [0 for _ in range(train_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Set\n",
    "x_test = real_content[train_size : train_size + test_size] + fake_content[train_size : train_size + test_size]\n",
    "y_test = [1 for _ in range(test_size)] + [0 for _ in range(test_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # Tokenize \n",
    "# x_train_tokens = [nltk.word_tokenize(sample) for sample in x_train]\n",
    "# x_test_tokens = [nltk.word_tokenize(sample) for sample in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54.9 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # Lemmatize\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# x_train_lemmatized = [[lemmatizer.lemmatize(word) for word in tokenized if word not in string.punctuation] for tokenized in x_train_tokens]\n",
    "# x_test_lemmatized = [[lemmatizer.lemmatize(word) for word in tokenized if word not in string.punctuation] for tokenized in x_test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train_lemmatized[:]\n",
    "# x_test = x_test_lemmatized[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TF-IDF Vector implementation.\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 4), stop_words='english')\n",
    "\n",
    "# Vectorize the training and testing datasets.\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "x_test_vectorized = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18463046"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.98      0.78      0.87      1000\n",
      "        Real       0.82      0.99      0.89      1000\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2000\n",
      "   macro avg       0.90      0.88      0.88      2000\n",
      "weighted avg       0.90      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_model = LinearSVC()\n",
    "SVM_model.fit(x_train_vectorized, y_train)\n",
    "print(classification_report(y_true = y_test, y_pred = SVM_model.predict(x_test_vectorized), target_names = [\"Fake\", \"Real\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.99      0.77      0.87      1000\n",
      "        Real       0.81      0.99      0.89      1000\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2000\n",
      "   macro avg       0.90      0.88      0.88      2000\n",
      "weighted avg       0.90      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(x_train_vectorized, y_train)\n",
    "print(classification_report(y_true = y_test, y_pred = NB_model.predict(x_test_vectorized), target_names = [\"Fake\", \"Real\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.98      0.74      0.84      1000\n",
      "        Real       0.79      0.98      0.88      1000\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      2000\n",
      "   macro avg       0.88      0.86      0.86      2000\n",
      "weighted avg       0.88      0.86      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_model = LogisticRegression(solver = 'lbfgs')\n",
    "LR_model.fit(x_train_vectorized, y_train)\n",
    "print(classification_report(y_true = y_test, y_pred = LR_model.predict(x_test_vectorized), target_names = [\"Fake\", \"Real\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"3\"\n",
    "\n",
    "# Save Models\n",
    "pickle.dump(SVM_model, open(\"models/News/SVM/news_model-\" + model_id + \".pl\",\"wb\"))\n",
    "pickle.dump(LR_model, open(\"models/News/LR/news_model-\" + model_id + \".pl\",\"wb\"))\n",
    "pickle.dump(NB_model, open(\"models/News/NB/news_model-\" + model_id + \".pl\",\"wb\"))\n",
    "\n",
    "# Save Vectors\n",
    "pickle.dump(vectorizer, open('vectors/News/vector-' + model_id + '.pl',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxicity Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_df = pd.read_csv(\"D:/Academic/Data Science/nc_data/toxic/train.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_df.loc[toxic_df['severe_toxic'] == 1,\"toxic\"] = 1\n",
    "# toxic_df.loc[toxic_df['obscene'] == 1,\"toxic\"] = 1\n",
    "# toxic_df.loc[toxic_df['threat'] == 1,\"toxic\"] = 1\n",
    "# toxic_df.loc[toxic_df['insult'] == 1,\"toxic\"] = 1\n",
    "# toxic_df.loc[toxic_df['identity_hate'] == 1,\"toxic\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toxic Vectors\n",
    "toxic = list(toxic_df.loc[toxic_df['toxic'] == 1].comment_text)\n",
    "nontoxic = list(toxic_df.loc[toxic_df['toxic'] == 0].comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15294, 144277)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toxic),len(nontoxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 4000\n",
    "test_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "x_train = toxic[:train_size] + nontoxic[:train_size]\n",
    "y_train = [1 for _ in range(train_size)] + [0 for _ in range(train_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Set\n",
    "x_test = toxic[train_size : train_size + test_size] + nontoxic[train_size : train_size + test_size]\n",
    "y_test = [1 for _ in range(test_size)] + [0 for _ in range(test_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF - IDF Vector\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "x_test_vectorized = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.86      0.88      0.87      2000\n",
      "        Real       0.88      0.86      0.87      2000\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      4000\n",
      "   macro avg       0.87      0.87      0.87      4000\n",
      "weighted avg       0.87      0.87      0.87      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_model = LinearSVC()\n",
    "SVM_model.fit(x_train_vectorized, y_train)\n",
    "print(classification_report(y_true = y_test, y_pred = SVM_model.predict(x_test_vectorized), target_names = [\"Fake\", \"Real\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.78      0.94      0.85      2000\n",
      "        Real       0.92      0.73      0.82      2000\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      4000\n",
      "   macro avg       0.85      0.84      0.83      4000\n",
      "weighted avg       0.85      0.84      0.83      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(x_train_vectorized, y_train)\n",
    "print(classification_report(y_true = y_test, y_pred = NB_model.predict(x_test_vectorized), target_names = [\"Fake\", \"Real\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.82      0.88      0.85      2000\n",
      "        Real       0.87      0.81      0.84      2000\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      4000\n",
      "   macro avg       0.85      0.84      0.84      4000\n",
      "weighted avg       0.85      0.84      0.84      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_model = LogisticRegression(solver = 'lbfgs')\n",
    "LR_model.fit(x_train_vectorized, y_train)\n",
    "print(classification_report(y_true = y_test, y_pred = LR_model.predict(x_test_vectorized), target_names = [\"Fake\", \"Real\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(LR_model, open(\"D:/Academic/Data Science/NewsClassifier/models/Toxic/model-1.toxic\",\"wb\"))\n",
    "pickle.dump(vectorizer, open(\"D:/Academic/Data Science/NewsClassifier/vectors/Toxic/vector-1.toxic\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article(object) :\n",
    "    def __init__(self, source, author, title, desc, content, date) :\n",
    "        self.source = source;\n",
    "        self.author = author;\n",
    "        self.title = title;\n",
    "        self.desc = desc;\n",
    "        self.content = content;\n",
    "        self.date = date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleObjects = pickle.load(open(\"articles.pl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions for the articles sent by News API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_news = vectorizer.transform([article.content for article in articleObjects if type(article.content) == type(\"\")])\n",
    "# import json\n",
    "# fulldb = [c for c in content if type(c) == type(\"\")]\n",
    "# x_news = vectorizer.transform(fulldb[:10000] + fake_content[11000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = SVM_model.predict(x_news)\n",
    "# y_true = [1 for _ in range(10000)] + [0 for _ in range(len(fake_content[11000:]))]\n",
    "\n",
    "# print(\"Accuracy:\", classification_report(y_pred = y_pred, y_true = y_true));\n",
    "# fig, ax = plt.subplots(1,1)\n",
    "# print(\"Confusion Matrix\");\n",
    "# sea.heatmap(confusion_matrix(y_pred = y_pred, y_true = y_true),annot=True,annot_kws={\"size\": 12},cmap='Blues', fmt='g', ax=ax, xticklabels=[\"Fake\",\"Real\"], yticklabels=[\"Fake\",\"Real\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = NB_model.predict(x_news)\n",
    "# y_true = [1 for _ in range(10000)] + [0 for _ in range(len(fake_content[11000:]))]\n",
    "\n",
    "# print(\"Accuracy:\", classification_report(y_pred = y_pred, y_true = y_true));\n",
    "# fig, ax = plt.subplots(1, 1)\n",
    "# print(\"Confusion Matrix\");\n",
    "# sea.heatmap(confusion_matrix(y_pred = y_pred, y_true = y_true),annot=True,annot_kws={\"size\": 12},cmap='Blues', fmt='g', ax=ax, xticklabels=[\"Fake\",\"Real\"], yticklabels=[\"Fake\",\"Real\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = LR_model.predict(x_news)\n",
    "# y_true = [1 for _ in range(10000)] + [0 for _ in range(len(fake_content[11000:]))]\n",
    "\n",
    "# print(\"Accuracy:\", classification_report(y_pred = y_pred, y_true = y_true));\n",
    "# fig, ax = plt.subplots(1, 1)\n",
    "# print(\"Confusion Matrix\");\n",
    "# sea.heatmap(confusion_matrix(y_pred = y_pred, y_true = y_true),annot=True,annot_kws={\"size\": 12},cmap='Blues', fmt='g', ax=ax, xticklabels=[\"Fake\",\"Real\"], yticklabels=[\"Fake\",\"Real\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
